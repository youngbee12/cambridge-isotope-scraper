#!/usr/bin/env python3
"""
ä¼˜åŒ–çš„å¤šçº¿ç¨‹Cambridge Isotope Laboratoriesçˆ¬è™«
æ”¹è¿›äº†æ•°æ®æå–å’Œ404é¡µé¢å¤„ç†
"""

import time
import json
import re
import csv
import random
import threading
import argparse
from concurrent.futures import ThreadPoolExecutor, as_completed
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import pandas as pd
from datetime import datetime
import logging
import requests

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class OptimizedMultithreadedScraper:
    def __init__(self, max_workers=2, headless=True):
        self.max_workers = max_workers
        self.headless = headless
        self.products = []
        self.failed_urls = []
        self.skipped_urls = []
        self.lock = threading.Lock()
        self.output_prefix = 'optimized_products'
        
        # è®¾ç½®Chromeé€‰é¡¹
        self.chrome_options = Options()
        if self.headless:
            self.chrome_options.add_argument("--headless")
        self.chrome_options.add_argument("--no-sandbox")
        self.chrome_options.add_argument("--disable-dev-shm-usage")
        self.chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        self.chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        self.chrome_options.add_experimental_option('useAutomationExtension', False)
        self.chrome_options.add_argument("--window-size=1920,1080")
        self.chrome_options.add_argument("--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
        
        # å¿«é€Ÿæ£€æŸ¥session
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        
    def create_driver(self):
        """ä¸ºæ¯ä¸ªçº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„WebDriverå®ä¾‹"""
        max_retries = 3
        for attempt in range(max_retries):
            try:
                service = Service(ChromeDriverManager().install())

                # ä¸ºé«˜çº¿ç¨‹æ•°æ·»åŠ é¢å¤–çš„Chromeé€‰é¡¹
                options = Options()
                for arg in self.chrome_options.arguments:
                    options.add_argument(arg)

                # é«˜çº¿ç¨‹æ•°æ—¶çš„é¢å¤–ä¼˜åŒ–
                if self.max_workers > 8:
                    options.add_argument("--disable-extensions")
                    options.add_argument("--disable-plugins")
                    options.add_argument("--disable-images")
                    options.add_argument("--disable-javascript")  # å¯èƒ½å½±å“æ•°æ®æå–
                    options.add_argument("--memory-pressure-off")
                    options.add_argument("--max_old_space_size=4096")

                driver = webdriver.Chrome(service=service, options=options)
                driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
                driver.set_page_load_timeout(30)
                return driver

            except Exception as e:
                logger.warning(f"åˆ›å»ºWebDriverå¤±è´¥ (å°è¯• {attempt+1}/{max_retries}): {e}")
                if attempt == max_retries - 1:
                    raise
                time.sleep(random.uniform(1, 3))  # éšæœºå»¶è¿Ÿåé‡è¯•
    
    def quick_check_page_status(self, url):
        """å¿«é€Ÿæ£€æŸ¥é¡µé¢çŠ¶æ€"""
        try:
            response = self.session.head(url, timeout=5, allow_redirects=True)
            if response.status_code == 404:
                return 'not_found'
            elif response.status_code >= 400:
                return 'error'
            elif response.status_code == 200:
                return 'ok'
            else:
                return 'unknown'
        except:
            return 'unknown'
    
    def extract_product_info_optimized(self, url):
        """ä¼˜åŒ–çš„äº§å“ä¿¡æ¯æå–"""
        thread_id = threading.current_thread().ident
        logger.info(f"[çº¿ç¨‹{thread_id}] å¤„ç†: {url}")
        
        # å¿«é€Ÿæ£€æŸ¥é¡µé¢çŠ¶æ€
        status = self.quick_check_page_status(url)
        if status == 'not_found':
            logger.info(f"[çº¿ç¨‹{thread_id}] å¿«é€Ÿè·³è¿‡404é¡µé¢: {url}")
            with self.lock:
                self.skipped_urls.append(url)
            return None
        
        driver = self.create_driver()
        
        try:
            # æ ¹æ®çº¿ç¨‹æ•°åŠ¨æ€è°ƒæ•´å»¶è¿Ÿ
            if self.max_workers > 16:
                initial_delay = random.uniform(8, 15)  # é«˜çº¿ç¨‹æ•°æ—¶æ›´é•¿å»¶è¿Ÿ
            elif self.max_workers > 8:
                initial_delay = random.uniform(5, 10)
            else:
                initial_delay = random.uniform(3, 6)

            driver.get(url)
            time.sleep(initial_delay)

            # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
            WebDriverWait(driver, 30).until(
                lambda d: d.execute_script("return document.readyState") == "complete"
            )

            # ç­‰å¾…React/JavaScriptå†…å®¹åŠ è½½
            time.sleep(8)  # å¢åŠ ç­‰å¾…æ—¶é—´è®©åŠ¨æ€å†…å®¹åŠ è½½

            # å°è¯•ç­‰å¾…ç‰¹å®šå…ƒç´ å‡ºç°
            try:
                WebDriverWait(driver, 10).until(
                    EC.any_of(
                        EC.presence_of_element_located((By.CSS_SELECTOR, ".Details_customHorizontal")),
                        EC.presence_of_element_located((By.CSS_SELECTOR, ".Details_customVertical")),
                        EC.presence_of_element_located((By.CSS_SELECTOR, "h1"))
                    )
                )
                time.sleep(3)  # é¢å¤–ç­‰å¾…ç¡®ä¿å†…å®¹å®Œå…¨åŠ è½½
            except TimeoutException:
                logger.warning(f"[çº¿ç¨‹{thread_id}] ç­‰å¾…é¡µé¢å…ƒç´ è¶…æ—¶ï¼Œç»§ç»­å°è¯•æå–")
            
            # æ£€æŸ¥404é¡µé¢
            page_title = driver.title.lower()
            if 'not found' in page_title or 'page cannot be found' in page_title:
                logger.info(f"[çº¿ç¨‹{thread_id}] Seleniumæ£€æµ‹åˆ°404é¡µé¢: {url}")
                with self.lock:
                    self.skipped_urls.append(url)
                return None
            
            product_info = {
                'url': url,
                'name': '',
                'product_number': '',
                'cas_labeled': '',
                'cas_unlabeled': '',
                'synonyms': '',
                'formula': '',
                'molecular_weight': '',
                'isotopic_enrichment': '',
                'chemical_purity': '',
                'description': '',
                'image_url': '',
                'page_title': driver.title
            }
            
            # æå–åŸºæœ¬ä¿¡æ¯
            product_info['name'] = self.extract_product_name(driver)
            product_info['product_number'] = self.extract_product_number(url)
            product_info['image_url'] = self.extract_product_image(driver)
            
            # ç­‰å¾…åŠ¨æ€å†…å®¹åŠ è½½
            time.sleep(2)
            
            # æå–è¯¦ç»†ä¿¡æ¯ - ä½¿ç”¨å¤šç§æ–¹æ³•
            self.extract_details_comprehensive(driver, product_info)

            # è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºé¡µé¢å†…å®¹
            logger.info(f"[çº¿ç¨‹{thread_id}] é¡µé¢æ ‡é¢˜: {driver.title}")
            logger.info(f"[çº¿ç¨‹{thread_id}] é¡µé¢URL: {driver.current_url}")

            # æ£€æŸ¥é¡µé¢æ˜¯å¦åŒ…å«äº§å“è¯¦æƒ…å…ƒç´ 
            details_elements = driver.find_elements(By.CSS_SELECTOR, ".Details_customHorizontal, .Details_customVertical")
            logger.info(f"[çº¿ç¨‹{thread_id}] æ‰¾åˆ° {len(details_elements)} ä¸ªè¯¦æƒ…å…ƒç´ ")

            # éªŒè¯æå–ç»“æœ
            if product_info['name'] or product_info['cas_labeled'] or product_info['formula']:
                logger.info(f"[çº¿ç¨‹{thread_id}] âœ… æˆåŠŸæå–: {product_info['name']} ({product_info['product_number']})")
                logger.info(f"[çº¿ç¨‹{thread_id}]   CAS: {product_info['cas_labeled']} / {product_info['cas_unlabeled']}")
                logger.info(f"[çº¿ç¨‹{thread_id}]   åˆ†å­å¼: {product_info['formula']}")
                return product_info
            else:
                logger.warning(f"[çº¿ç¨‹{thread_id}] âš ï¸  æå–çš„æ•°æ®ä¸ºç©º: {url}")
                logger.warning(f"[çº¿ç¨‹{thread_id}]   é¡µé¢æ ‡é¢˜: {driver.title}")

                # ä¿å­˜é¡µé¢æºç ç”¨äºè°ƒè¯•
                try:
                    with open(f'debug_page_{thread_id}_{int(time.time())}.html', 'w', encoding='utf-8') as f:
                        f.write(driver.page_source)
                    logger.info(f"[çº¿ç¨‹{thread_id}] é¡µé¢æºç å·²ä¿å­˜ç”¨äºè°ƒè¯•")
                except:
                    pass

                return product_info  # ä»ç„¶è¿”å›ï¼Œå³ä½¿æ•°æ®ä¸ºç©º
            
        except Exception as e:
            logger.error(f"[çº¿ç¨‹{thread_id}] æå–å¤±è´¥ {url}: {e}")
            with self.lock:
                self.failed_urls.append(url)
            return None
        
        finally:
            driver.quit()
    
    def extract_product_name(self, driver):
        """æå–äº§å“åç§°"""
        name_selectors = [
            'h1',
            '.product-title',
            '.product-name',
            '.page-title',
            '.entry-title',
            '[data-testid="product-name"]'
        ]
        
        for selector in name_selectors:
            try:
                elements = driver.find_elements(By.CSS_SELECTOR, selector)
                for element in elements:
                    text = element.text.strip()
                    if text and len(text) > 3 and 'Cambridge Isotope' not in text and 'not found' not in text.lower():
                        return text
            except:
                continue
        
        # ä»é¡µé¢æ ‡é¢˜æå–
        try:
            title = driver.title
            if title and 'Cambridge Isotope' in title and 'not found' not in title.lower():
                # æå–äº§å“åç§°éƒ¨åˆ†
                name_match = re.search(r'^([^-]+)', title)
                if name_match:
                    name = name_match.group(1).strip()
                    if len(name) > 3:
                        return name
        except:
            pass
        
        return ''
    
    def extract_product_number(self, url):
        """ä»URLæå–äº§å“ç¼–å·"""
        patterns = [
            r'([cdno]lm-\d+(?:-[a-z0-9]+)?)',
            r'itemno=([A-Z0-9-]+)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, url, re.IGNORECASE)
            if match:
                return match.group(1).upper()
        
        return ''
    
    def extract_product_image(self, driver):
        """æå–äº§å“å›¾ç‰‡URL"""
        try:
            # ç­‰å¾…å›¾ç‰‡åŠ è½½
            time.sleep(1)
            
            image_selectors = [
                'img[src*="/product/image/"]',
                'img[src*="cdlm-"]',
                'img[src*="clm-"]',
                'img[src*="dlm-"]',
                'img[src*="nlm-"]',
                'img[src*="olm-"]',
                '.product-image img',
                '.product-photo img'
            ]
            
            for selector in image_selectors:
                try:
                    elements = driver.find_elements(By.CSS_SELECTOR, selector)
                    for element in elements:
                        src = element.get_attribute('src')
                        if src and '/product/image/' in src:
                            if src.startswith('/'):
                                src = 'https://isotope.com' + src
                            return src
                except:
                    continue
        except:
            pass
        
        return ''
    
    def extract_details_comprehensive(self, driver, product_info):
        """ç»¼åˆæå–è¯¦ç»†ä¿¡æ¯"""
        try:
            # æ–¹æ³•1: CSSé€‰æ‹©å™¨æå–
            self.extract_details_by_css(driver, product_info)
            
            # æ–¹æ³•2: å¦‚æœCSSæ–¹æ³•æ²¡æœ‰è·å–åˆ°ä¸»è¦ä¿¡æ¯ï¼Œä½¿ç”¨é¡µé¢æºç æå–
            if not product_info['cas_labeled'] and not product_info['cas_unlabeled']:
                self.extract_details_from_source(driver, product_info)
            
            # æ–¹æ³•3: å°è¯•ç­‰å¾…å¹¶é‡æ–°æå–
            if not product_info['cas_labeled'] and not product_info['formula']:
                time.sleep(2)
                self.extract_details_by_css(driver, product_info)
        
        except Exception as e:
            logger.warning(f"è¯¦ç»†ä¿¡æ¯æå–å¤±è´¥: {e}")
    
    def extract_details_by_css(self, driver, product_info):
        """ä½¿ç”¨CSSé€‰æ‹©å™¨æå–è¯¦ç»†ä¿¡æ¯"""
        thread_id = threading.current_thread().ident
        extracted_count = 0

        try:
            detail_selectors = ['.Details_customHorizontal', '.Details_customVertical']

            for selector in detail_selectors:
                try:
                    elements = driver.find_elements(By.CSS_SELECTOR, selector)
                    logger.info(f"[çº¿ç¨‹{thread_id}] æ‰¾åˆ° {len(elements)} ä¸ª {selector} å…ƒç´ ")

                    for i, element in enumerate(elements):
                        try:
                            name_element = element.find_element(By.CSS_SELECTOR, '.Details_name')
                            name = name_element.text.strip().lower()

                            spans = element.find_elements(By.TAG_NAME, 'span')
                            if len(spans) >= 2:
                                value = spans[1].text.strip()
                                logger.info(f"[çº¿ç¨‹{thread_id}] å…ƒç´ {i}: '{name}' = '{value}'")

                                if 'cas number labeled' in name:
                                    product_info['cas_labeled'] = value
                                    extracted_count += 1
                                elif 'cas number unlabeled' in name:
                                    product_info['cas_unlabeled'] = value
                                    extracted_count += 1
                                elif 'formula' in name:
                                    product_info['formula'] = value
                                    extracted_count += 1
                                elif 'synonyms' in name and not product_info['synonyms']:
                                    product_info['synonyms'] = value
                                    extracted_count += 1
                                elif 'molecular weight' in name:
                                    product_info['molecular_weight'] = value
                                    extracted_count += 1
                                elif 'enrichment' in name:
                                    product_info['isotopic_enrichment'] = value
                                    extracted_count += 1
                                elif 'purity' in name:
                                    product_info['chemical_purity'] = value
                                    extracted_count += 1
                            else:
                                logger.warning(f"[çº¿ç¨‹{thread_id}] å…ƒç´ {i}æ²¡æœ‰è¶³å¤Ÿçš„span: {len(spans)}")
                        except Exception as e:
                            logger.warning(f"[çº¿ç¨‹{thread_id}] å¤„ç†å…ƒç´ {i}å¤±è´¥: {e}")
                            continue
                except Exception as e:
                    logger.warning(f"[çº¿ç¨‹{thread_id}] æŸ¥æ‰¾{selector}å¤±è´¥: {e}")
                    continue

            logger.info(f"[çº¿ç¨‹{thread_id}] CSSæå–å®Œæˆï¼Œå…±æå– {extracted_count} ä¸ªå­—æ®µ")

        except Exception as e:
            logger.error(f"[çº¿ç¨‹{thread_id}] CSSæå–æ€»ä½“å¤±è´¥: {e}")
    
    def extract_details_from_source(self, driver, product_info):
        """ä»é¡µé¢æºç æå–è¯¦ç»†ä¿¡æ¯"""
        try:
            page_source = driver.page_source
            
            # CASå·æå–
            cas_patterns = [
                (r'>CAS Number Labeled</span><span>(\d{1,7}-\d{2}-\d)</span>', 'labeled'),
                (r'>CAS Number Unlabeled</span><span>(\d{1,7}-\d{2}-\d)</span>', 'unlabeled'),
                (r'CAS\s*Number\s*Labeled[:\s]*(\d{1,7}-\d{2}-\d)', 'labeled'),
                (r'CAS\s*Number\s*Unlabeled[:\s]*(\d{1,7}-\d{2}-\d)', 'unlabeled'),
            ]
            
            for pattern, cas_type in cas_patterns:
                matches = re.findall(pattern, page_source, re.IGNORECASE)
                if matches:
                    if cas_type == 'labeled':
                        product_info['cas_labeled'] = matches[0]
                    elif cas_type == 'unlabeled':
                        product_info['cas_unlabeled'] = matches[0]
            
            # åˆ†å­å¼æå–
            formula_patterns = [
                r'>Formula</span><span>([^<]+)</span>',
                r'Formula[:\s]*([A-Z][a-z]?(?:\d+)?(?:[A-Z*][a-z]?(?:\d+)?)*)',
            ]
            
            for pattern in formula_patterns:
                matches = re.findall(pattern, page_source, re.IGNORECASE)
                if matches:
                    product_info['formula'] = matches[0].strip()
                    break
        except:
            pass
    
    def scrape_products_multithreaded(self, urls, max_products=None):
        """å¤šçº¿ç¨‹çˆ¬å–äº§å“"""
        if max_products:
            urls = urls[:max_products]
        
        logger.info(f"å¼€å§‹å¤šçº¿ç¨‹çˆ¬å– {len(urls)} ä¸ªäº§å“ï¼Œä½¿ç”¨ {self.max_workers} ä¸ªçº¿ç¨‹")
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_url = {executor.submit(self.extract_product_info_optimized, url): url for url in urls}
            
            for future in as_completed(future_to_url):
                url = future_to_url[future]
                try:
                    result = future.result()
                    if result:
                        with self.lock:
                            self.products.append(result)
                            logger.info(f"è¿›åº¦: {len(self.products)} ä¸ªäº§å“å·²å®Œæˆ")
                except Exception as e:
                    logger.error(f"å¤„ç† {url} æ—¶å‡ºé”™: {e}")
                    with self.lock:
                        self.failed_urls.append(url)
        
        logger.info(f"å¤šçº¿ç¨‹çˆ¬å–å®Œæˆï¼")
        logger.info(f"æˆåŠŸ: {len(self.products)} ä¸ª")
        logger.info(f"è·³è¿‡: {len(self.skipped_urls)} ä¸ª")
        logger.info(f"å¤±è´¥: {len(self.failed_urls)} ä¸ª")
    
    def save_results(self):
        """ä¿å­˜ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        if self.products:
            # ä¿å­˜ä¸ºCSV
            csv_filename = f'{self.output_prefix}_{timestamp}.csv'
            df = pd.DataFrame(self.products)
            df.to_csv(csv_filename, index=False, encoding='utf-8')
            logger.info(f"CSVæ–‡ä»¶å·²ä¿å­˜: {csv_filename}")

            # ä¿å­˜ä¸ºExcel
            excel_filename = f'{self.output_prefix}_{timestamp}.xlsx'
            df.to_excel(excel_filename, index=False, engine='openpyxl')
            logger.info(f"Excelæ–‡ä»¶å·²ä¿å­˜: {excel_filename}")
        
        # ä¿å­˜è·³è¿‡å’Œå¤±è´¥çš„URL
        if self.skipped_urls:
            with open(f'skipped_urls_{timestamp}.txt', 'w', encoding='utf-8') as f:
                for url in self.skipped_urls:
                    f.write(url + '\n')
        
        if self.failed_urls:
            with open(f'failed_urls_{timestamp}.txt', 'w', encoding='utf-8') as f:
                for url in self.failed_urls:
                    f.write(url + '\n')
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        print(f"\n=== ä¼˜åŒ–å¤šçº¿ç¨‹çˆ¬å–ç»“æœ ===")
        print(f"æ€»å¤„ç†æ•°é‡: {len(self.products) + len(self.skipped_urls) + len(self.failed_urls)}")
        print(f"æˆåŠŸè·å–: {len(self.products)}")
        print(f"è·³è¿‡404é¡µé¢: {len(self.skipped_urls)}")
        print(f"å¤±è´¥: {len(self.failed_urls)}")
        
        if self.products:
            df = pd.DataFrame(self.products)
            print(f"\næ•°æ®è´¨é‡:")
            print(f"æœ‰äº§å“åç§°: {len(df[df['name'] != ''])}")
            print(f"æœ‰CAS Labeled: {len(df[df['cas_labeled'] != ''])}")
            print(f"æœ‰CAS Unlabeled: {len(df[df['cas_unlabeled'] != ''])}")
            print(f"æœ‰åˆ†å­å¼: {len(df[df['formula'] != ''])}")
            print(f"æœ‰å›¾ç‰‡: {len(df[df['image_url'] != ''])}")
            
            return csv_filename, excel_filename
        
        return None, None

def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='Cambridge Isotope Laboratories å¤šçº¿ç¨‹çˆ¬è™«')

    parser.add_argument('-t', '--threads', type=int, default=2,
                       help='çº¿ç¨‹æ•°é‡ (é»˜è®¤: 2, å»ºè®®: 2-8, æœ€å¤§: 128)')

    parser.add_argument('-n', '--max-products', type=int, default=None,
                       help='æœ€å¤§çˆ¬å–äº§å“æ•°é‡ (é»˜è®¤: æ— é™åˆ¶)')

    parser.add_argument('--headless', action='store_true',
                       help='ä½¿ç”¨headlessæ¨¡å¼ (é»˜è®¤: False)')

    parser.add_argument('-u', '--urls-file', type=str, default=None,
                       help='åŒ…å«URLåˆ—è¡¨çš„æ–‡ä»¶è·¯å¾„ (é»˜è®¤: ä½¿ç”¨å†…ç½®æµ‹è¯•URL)')

    parser.add_argument('-o', '--output-prefix', type=str, default='optimized_products',
                       help='è¾“å‡ºæ–‡ä»¶å‰ç¼€ (é»˜è®¤: optimized_products)')

    parser.add_argument('-v', '--verbose', action='store_true',
                       help='è¯¦ç»†è¾“å‡ºæ¨¡å¼')

    return parser.parse_args()

def load_urls_from_file(file_path):
    """ä»æ–‡ä»¶åŠ è½½URLåˆ—è¡¨"""
    urls = []
    try:
        if file_path.endswith('.json'):
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if isinstance(data, list):
                    # å¦‚æœæ˜¯URLå­—ç¬¦ä¸²åˆ—è¡¨
                    if all(isinstance(item, str) for item in data):
                        urls = data
                    # å¦‚æœæ˜¯åŒ…å«URLçš„å¯¹è±¡åˆ—è¡¨
                    else:
                        urls = [item.get('url', '') for item in data if item.get('url')]
        else:
            # æ–‡æœ¬æ–‡ä»¶ï¼Œæ¯è¡Œä¸€ä¸ªURL
            with open(file_path, 'r', encoding='utf-8') as f:
                urls = [line.strip() for line in f if line.strip()]

        logger.info(f"ä»æ–‡ä»¶ {file_path} åŠ è½½äº† {len(urls)} ä¸ªURL")
        return urls

    except Exception as e:
        logger.error(f"åŠ è½½URLæ–‡ä»¶å¤±è´¥: {e}")
        return []

def main():
    """ä¸»å‡½æ•°"""
    args = parse_arguments()

    # éªŒè¯çº¿ç¨‹æ•°å¹¶ç»™å‡ºè­¦å‘Š
    if args.threads > 16:
        print(f"âš ï¸  è­¦å‘Š: ä½¿ç”¨ {args.threads} ä¸ªçº¿ç¨‹å¯èƒ½å¯¼è‡´ä»¥ä¸‹é—®é¢˜:")
        print(f"   - ç½‘ç«™å¯èƒ½å°ç¦æ‚¨çš„IPåœ°å€")
        print(f"   - ç³»ç»Ÿèµ„æºæ¶ˆè€—è¿‡å¤§ (é¢„è®¡éœ€è¦ {args.threads * 200}MB+ å†…å­˜)")
        print(f"   - å¯èƒ½è¢«è§†ä¸ºDDoSæ”»å‡»")

        if args.threads > 64:
            print(f"âŒ å¼ºçƒˆä¸å»ºè®®ä½¿ç”¨è¶…è¿‡64ä¸ªçº¿ç¨‹!")
            response = input("æ˜¯å¦ç»§ç»­? (è¾“å…¥ 'yes' ç»§ç»­): ")
            if response.lower() != 'yes':
                print("å·²å–æ¶ˆæ‰§è¡Œ")
                return

    elif args.threads > 8:
        print(f"âš ï¸  æ³¨æ„: {args.threads} ä¸ªçº¿ç¨‹è¾ƒå¤šï¼Œå»ºè®®ç›‘æ§ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ")

    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    print(f"\n=== Cambridge Isotope Laboratories å¤šçº¿ç¨‹çˆ¬è™« ===")
    print(f"çº¿ç¨‹æ•°: {args.threads}")
    print(f"Headlessæ¨¡å¼: {args.headless}")
    print(f"æœ€å¤§äº§å“æ•°: {args.max_products or 'æ— é™åˆ¶'}")
    print(f"è¾“å‡ºå‰ç¼€: {args.output_prefix}")

    # è·å–URLåˆ—è¡¨
    if args.urls_file:
        urls = load_urls_from_file(args.urls_file)
        if not urls:
            print("é”™è¯¯: æ— æ³•ä»æ–‡ä»¶åŠ è½½URL")
            return
    else:
        # ä½¿ç”¨å†…ç½®æµ‹è¯•URL
        urls = [
            "https://isotope.com/carbohydrates/d-glucose-1-13c-6-13c-6-6-d2-cdlm-4895",
            "https://isotope.com/minimal-media-reagents/d-glucose-u-13c6-clm-1396-1",
            "https://isotope.com/dimethyl-sulfoxide-d6-dlm-10-10",
            "https://isotope.com/amino-acids/free-amino-acids/l-alanine-1-13c-clm-116-pk",
            "https://isotope.com/chloroform-d-dlm-7-10",  # 404
            "https://isotope.com/methanol-d4-dlm-24-10",  # 404
        ]
        print(f"ä½¿ç”¨å†…ç½®æµ‹è¯•URL: {len(urls)} ä¸ª")

    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    scraper = OptimizedMultithreadedScraper(
        max_workers=args.threads,
        headless=args.headless
    )

    # è®¾ç½®è¾“å‡ºå‰ç¼€
    scraper.output_prefix = args.output_prefix

    try:
        print(f"\nå¼€å§‹çˆ¬å–...")
        scraper.scrape_products_multithreaded(urls, max_products=args.max_products)
        csv_file, excel_file = scraper.save_results()

        if csv_file:
            print(f"\nâœ… å¤šçº¿ç¨‹çˆ¬å–å®Œæˆï¼")
            print(f"ğŸ“ æ–‡ä»¶å·²ä¿å­˜:")
            print(f"   - {csv_file}")
            print(f"   - {excel_file}")
        else:
            print(f"\nâš ï¸  æ²¡æœ‰æˆåŠŸçˆ¬å–åˆ°æ•°æ®")

    except KeyboardInterrupt:
        print(f"\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­äº†çˆ¬å–è¿‡ç¨‹")
        scraper.save_results()
    except Exception as e:
        print(f"\nâŒ çˆ¬å–è¿‡ç¨‹å‡ºé”™: {e}")
        scraper.save_results()

if __name__ == "__main__":
    main()
